{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multiclass linear classification\n",
    "<a id=part3></a>\n",
    "$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial {#1}}{\\partial {#2}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll learn about loss functions and how to optimize them with gradient descent.\n",
    "We'll then use this knowledge to train a very simple model: a linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "torch.random.manual_seed(1904)\n",
    "test = unittest.TestCase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classification\n",
    "\n",
    "<a id=part3_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi-class linear classification we have $C$ classes which we assume our samples\n",
    "may belong to.\n",
    "We apply a linear function to a sample $\\vec{x} \\in \\set{R}^{D}$ and obtain a score $s_j$ which\n",
    "represents how well $x$ fits the class $1\\leq j\\leq C$ according to our model:\n",
    "$$\n",
    "s_j = \\vectr{w_j} \\vec{x} + b_j.\n",
    "$$\n",
    "\n",
    "Note that we have a different set of model parameters (weights) $\\vec{w_j},~b_j$ for each class,\n",
    "so a total of $C\\cdot(D+1)$ parameters.\n",
    "\n",
    "To classify a sample, we simply calculate the score for each class and choose the class with the\n",
    "highest score as our prediction.\n",
    "\n",
    "One interpretation of the weights $\\vec{w_j},~b_j$ is that they represent the parameters of an\n",
    "$N$-dimensional hyperplane. Under this interpretation the class score $s_j$ of a sample is proportional\n",
    "to the distance of that sample from the hyperplane representing the $j$-th class. Note that this score\n",
    "can be positive or negative (depending on which side of the hyperplane the sample is).\n",
    "Such a classifier therefore splits the sample space into regions where the farther a sample is from the\n",
    "positive side of a hyperplane for class $j$, the higher $s_j$, so the more likely it belongs to class $j$.\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://waterprogramming.files.wordpress.com/2018/09/hyperplane1-e1538161721590.png\" width=\"400\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "In the context of supervised learning of a linear classifier model, we map a dataset\n",
    "(or batch from a dataset) of $N$ samples (for example, images flattened to vectors of length $D$)\n",
    "to a score for one of each of $C$ possible classes using the linear function above.\n",
    "\n",
    "To make the implementation efficient, we'll represent the mapping with a single matrix multiplication,\n",
    "employing the \"Bias trick\": \n",
    "Instead of both $\\vec{w_j}$ and $b_j$ per class, we'll put the bias term at the\n",
    "beginning of the weight vector and add a term $1$ at the start of each sample.\n",
    "\n",
    "The class scores for each sample are then given by:\n",
    "\n",
    "$$\n",
    "\\mat{S} = \\mat{X} \\mat{W}\n",
    "$$\n",
    "\n",
    "Where here (and in the code examples you'll work with),\n",
    "- $\\mat{X}$ is a matrix of shape $N\\times (D+1)$ containing $N$ samples in it's rows;\n",
    "- $\\mat{W}$ is of shape $(D+1)\\times C$ and contains the learnable classifier parameters (weights and bias);\n",
    "- $\\mat{S}$ is therefore a $N\\times C$ matrix of the class scores of each sample.\n",
    "\n",
    "Notes: \n",
    "1. In the following discussions we'll use the notation $\\vec{x_i}$ to denote the $i$-th training sample\n",
    "   (row $i$ in $\\mat{X}$) and $\\vec{w_j}$ to denote the weights and bias for class $j$ (column $j$ in $\\mat{W}$).\n",
    "   However, when writing explicit vectors we treat them all as columns, so e.g. $\\vectr{w_j}\\vec{x_i}$ is an\n",
    "   inner product.\n",
    "2. The reason we put the samples in the rows of $\\mat{X}$ and not columns (as is the convention in some texts) is\n",
    "   because that's the convention in the pytorch library: the batch dimension is always the first one. This has many\n",
    "   implementation advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Implement the `BiasTrick` transform class in the module `hw1/transforms.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape = torch.Size([64, 513])\n",
      "shape = torch.Size([2, 3, 4, 5, 6, 8])\n",
      "shape = torch.Size([1, 13])\n",
      "shape = torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import hw1.transforms as hw1tf\n",
    "\n",
    "tf_btrick = hw1tf.BiasTrick()\n",
    "\n",
    "test_cases = [\n",
    "    torch.randn(64, 512),\n",
    "    torch.randn(2, 3, 4, 5, 6, 7),\n",
    "    torch.randint(low=0, high=10, size=(1, 12)),\n",
    "    torch.tensor([10, 11, 12])\n",
    "]\n",
    "\n",
    "for x_test in test_cases:\n",
    "    xb = tf_btrick(x_test)\n",
    "    print('shape =', xb.shape)\n",
    "    test.assertEqual(x_test.dtype, xb.dtype, \"Wrong dtype\")\n",
    "    test.assertTrue(torch.all(xb[..., 1:] == x_test), \"Original features destroyed\")\n",
    "    test.assertTrue(torch.all(xb[..., [0]] == torch.ones(*xb.shape[:-1], 1)), \"First feature is not equal to 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as tvtf\n",
    "\n",
    "# Define the transforms that should be applied to each image in the dataset before returning it\n",
    "tf_ds = tvtf.Compose([\n",
    "    # Convert PIL image to pytorch Tensor\n",
    "    tvtf.ToTensor(),\n",
    "    # Normalize each chanel with precomputed mean and std of the train set\n",
    "    tvtf.Normalize(mean=(0.1307,), std=(0.3081,)),\n",
    "    # Reshape to 1D Tensor\n",
    "    hw1tf.TensorView(-1), \n",
    "    # Apply the bias trick (add bias element to features)\n",
    "    hw1tf.BiasTrick(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will use your transform to load a subset of the [MNIST](http://yann.lecun.com/exdb/mnist/)\n",
    "dataset for us to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hw1.datasets as hw1datasets\n",
    "import hw1.dataloaders as hw1dataloaders\n",
    "\n",
    "# Define how much data to load\n",
    "num_train = 10000\n",
    "num_test = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "# Training dataset\n",
    "data_root = os.path.expanduser('~/.pytorch-datasets')\n",
    "ds_train = hw1datasets.SubsetDataset(\n",
    "    torchvision.datasets.MNIST(root=data_root, download=True, train=True, transform=tf_ds),\n",
    "    num_train)\n",
    "\n",
    "# Create training & validation sets\n",
    "dl_train, dl_valid = hw1dataloaders.create_train_validation_loaders(\n",
    "    ds_train, validation_ratio=0.2, batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Test dataset & loader\n",
    "ds_test = hw1datasets.SubsetDataset(\n",
    "    torchvision.datasets.MNIST(root=data_root, download=True, train=False, transform=tf_ds),\n",
    "    num_test)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size)\n",
    "\n",
    "x0, y0 = ds_train[0]\n",
    "n_features = torch.numel(x0)\n",
    "n_classes = 10\n",
    "\n",
    "# Make sure samples have bias term added\n",
    "test.assertEqual(n_features, 28*28*1+1, \"Incorrect sample dimension\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Complete the implementation of the `__init()__`, `predict()` and `evaluate_accuracy()` functions in the\n",
    "`LinearClassifier` class located in the `hw1/linear_classifier.py` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 6.4%\n"
     ]
    }
   ],
   "source": [
    "import hw1.linear_classifier as hw1linear\n",
    "\n",
    "# Create a classifier\n",
    "lin_cls = hw1linear.LinearClassifier(n_features, n_classes)\n",
    "\n",
    "# Evaluate accuracy on test set\n",
    "mean_acc = 0\n",
    "for (x,y) in dl_test:\n",
    "    y_pred, _ = lin_cls.predict(x)\n",
    "    mean_acc += lin_cls.evaluate_accuracy(y, y_pred)\n",
    "mean_acc /= len(dl_test)\n",
    "\n",
    "print(f\"Accuracy: {mean_acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get an accuracy of around 10%, corresponding to a random guess of one of ten classes. You can run the above code block multiple times to sample different initial weights and get slightly different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "<a id=part3_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that a linear model computes the class scores for each sample using a linear mapping as\n",
    "a score function.\n",
    "However in order to train the model, we need to define  some measure of how\n",
    "well we've classified our samples compared to their ground truth labels.\n",
    "This measure is known as a **loss function**, and it's selection is crucial in determining the model\n",
    "that will result from training. A loss function produces lower values the better the classification is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass SVM loss function\n",
    "\n",
    "A very common linear model for classification is the Support Vector Machine. An SVM attempts to find\n",
    "separating hyperplanes that have the property of creating a maximal margin to the training samples, i.e.\n",
    "hyperplanes that are as far as possible from the closest training samples.\n",
    "For example, in the following image we see a simple case with two classes of samples that have only two features.\n",
    "The data is linearly separable and it's easy to see there are infinite possible hyperplanes (in this case lines)\n",
    "that separate the data perfectly.\n",
    "\n",
    "The SVM model finds the optimal hyperplane, which is the one with\n",
    "the maximal margin. The data points closest to the separating hyperplane are called the Support Vectors\n",
    "(it can be shown that only they determine the hyperplane).\n",
    "We can see that the width of the margin is $\\frac{2}{\\norm{\\vec{w}}}$. In this simple case since the data is linearly\n",
    "separable, there exists a solution where no samples fall within the margin. If the data is not linearly separable, we\n",
    "need to allow samples to enter the margin (with a cost). This is known as a soft-margin SVM.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/2a/Svm_max_sep_hyperplane_with_margin.png\" width=400 alt=\"svm\"/>\n",
    "\n",
    "There are many ways to train an SVM model. Classically, the problem is stated as constrained optimization and\n",
    "solved with quadratic optimization techniques.\n",
    "In this exercise, we'll instead work directly with the uncontrained SVM loss function,\n",
    "calculate it's gradient analytically, and then minimize it with gradient descent.\n",
    "As we'll see in the rest of the course, this technique will be a\n",
    "major component when we train deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The **in-sample** (empirical) loss function for a multiclass soft-margin SVM can be stated as follows:\n",
    "\n",
    "$$\n",
    "L(\\mat{W}) =\n",
    "\\frac{1}{N} \\sum_{i=1}^{N} L_{i}(\\mat{W})\n",
    "+\n",
    "\\frac{\\lambda}{2} \\norm{\\mat{W}}^2\n",
    "$$\n",
    "\n",
    "Where the first term is the mean pointwise data-dependent loss $L_{i}$,\n",
    "given by the [hinge loss](https://en.wikipedia.org/wiki/Hinge_loss) formula,\n",
    "\n",
    "$$\n",
    "L_{i}(\\mat{W}) =  \\sum_{j \\neq y_i} \\max\\left(0, \\Delta+ \\vectr{w_j} \\vec{x_i} - \\vectr{w_{y_i}} \\vec{x_i}\\right),\n",
    "$$\n",
    "\n",
    "and the second term is a regularization loss which depends only on model parameters.\n",
    "Note that the hinge loss term sums over the *wrong* class prediction scores for each sample:\n",
    "$j\\neq y_i$, and $y_i$ is the ground-truth label for sample $i$.\n",
    "This can be understood as attempting to make sure that the score for the correct class is higher than the other \n",
    "classes by\n",
    "at least some margin $\\Delta > 0$, otherwise a loss is incurred.\n",
    "This way, we allow samples to fall within the margin but incur loss, which gives us a soft-margin SVM.\n",
    "\n",
    "The regularization term penalizes large weight magnitudes to prevent ambiguous solutions since if \n",
    "e.g. $\\mat{W^*}$ is a weight matrix that perfectly separates the data, so is $\\alpha\\mat{W^*}$ for\n",
    "any scalar $\\alpha \\geq 1$.\n",
    "\n",
    "Fitting an SVM model then amounts to finding the weight matrix $\\mat{W}$ which minimizes $L(\\mat{W})$.\n",
    "Note that we're writing the loss as a function of $\\mat{W}$ to\n",
    "emphasize that we wish to minimize it's value on the given data by with respect to the weights $\\mat{W}$,\n",
    "even though it obviously depends also on our specific dataset, $\\left\\{ \\vec{x_i}, y_i \\right\\}_{i=1}^{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "**TODO** Implement the SVM hinge loss function in the module `hw1/losses.py`, within the `SVMHingeLoss` class.\n",
    "Implement just the `loss()` function. For now you can ignore the part about saving tensors for the gradient calculation. Run the following to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 9.023382186889648\n",
      "diff = 8.218688964767296e-05\n"
     ]
    }
   ],
   "source": [
    "import cs3600.dataloader_utils as dl_utils\n",
    "from hw1.losses import SVMHingeLoss\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "# Classify all samples in the test set\n",
    "# because it doesn't depend on randomness of train/valid split\n",
    "x, y = dl_utils.flatten(dl_test)\n",
    "\n",
    "# Compute predictions\n",
    "lin_cls = hw1linear.LinearClassifier(n_features, n_classes)\n",
    "y_pred, x_scores = lin_cls.predict(x)\n",
    "\n",
    "# Calculate loss with our hinge-loss implementation\n",
    "loss_fn = SVMHingeLoss(delta=1.)\n",
    "loss = loss_fn(x, y, x_scores, y_pred)\n",
    "\n",
    "# Compare to pre-computed expected value as a test\n",
    "expected_loss = 9.0233\n",
    "print(\"loss =\", loss.item())\n",
    "print('diff =', abs(loss.item()-expected_loss))\n",
    "test.assertAlmostEqual(loss.item(), expected_loss, delta=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing a Loss Function with Gradient Descent\n",
    "<a id=part3_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll implement a simple gradient descent optimizer for the loss function we've implemented above. As you have seen in the lectures, the basic gradient-based optimization scheme is as follows:\n",
    "\n",
    "1. Start with initial model weights $\\mat{W_0}$ initialized randomly.\n",
    "1. For $k=1,2,\\dots,K$:\n",
    "    1. Select a step size $\\eta_k$.\n",
    "    1. Compute the gradient of the loss w.r.t. $\\mat{W}$ and evaluate at the current weights:\n",
    "        $\\nabla_{\\mat{W}} L(\\mat{W_{k-1}})$.\n",
    "    1. Update: \n",
    "        $$\n",
    "        \\mat{W_k} = \\mat{W_{k-1}} - \\eta_k \\nabla_{\\mat{W}} L(\\mat{W_{k-1}})\n",
    "        $$\n",
    "    1. Stop if minimum reached or validation-set loss is low enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crucial component here is the gradient calculation.\n",
    "In this exercise we'll analytically derive the gradient\n",
    "of the loss and then implement it in code.\n",
    "In the next parts of the course we'll enjoy the automatic-differentiation features of PyTorch,\n",
    "but for now we'll do it the old-fashioned way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important detail to note is that while $L(\\mat{W})$ is scalar-valued, it's a function of all the elements of the\n",
    "matrix $\\mat{W}$. Therefore it's gradient w.r.t. $\\mat{W}$ is also a matrix of the same shape as $\\mat{W}$:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mat{W}} L =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial L}{\\partial W_{1,1}} & & \\cdots & \\frac{\\partial L}{\\partial W_{1,C}} \\\\\n",
    "    \\frac{\\partial L}{\\partial W_{2,1}} & \\ddots &  \\\\\n",
    "    \\vdots & & \\ddots &  \\\\\n",
    "    \\frac{\\partial L}{\\partial W_{D+1,1}} & \\cdots &  & \\frac{\\partial L}{\\partial W_{D+1,C}} \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert \\\\\n",
    "\\frac{\\partial L}{\\partial\\vec{w_1}} & \\cdots & \\frac{\\partial L}{\\partial\\vec{w_C}}\\\\\n",
    "\\vert & & \\vert \\\\\n",
    "\\end{bmatrix}\n",
    "\\in \\set{R}^{(D+1)\\times C}.\n",
    "$$\n",
    "\n",
    "For our gradient descent update-step we'll need to create such a matrix of derivatives and evaluate it at the \n",
    "current value of the weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM loss gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is formulate an expression for the gradient of the loss function defined above. Since the expression for the loss depends on the columns of $\\mat{W}$, we'll derive an expression for the gradient of $L(\\mat{W})$ w.r.t. each $\\vec{w_j}$:\n",
    "\n",
    "$$\n",
    "\\pderiv{L}{\\vec{w_j}}(\\mat{W}) = \n",
    "\\frac{1}{N} \\sum_{i=1}^{N} \\pderiv{L_{i}}{\\vec{w_j}}(\\mat{W})\n",
    "+\n",
    "\\lambda \\vec{w_j}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the gradient of the pointwise loss, let's define the **margin-loss** of sample $i$ for class $j$\n",
    "as follows: $m_{i,j} = \\Delta + \\vectr{w_j}\\vec{x_i} - \\vectr{w_{y_i}}\\vec{x_i}$.\n",
    "We can then write the pointwise loss and it's gradient in terms of $m_{i,j}$. We'll separate the case of $j=y_i$\n",
    "(i.e. the gradient for the correct class):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\pderiv{L_i}{\\vec{w_j}} & =\n",
    "        \\begin{cases}\n",
    "            \\vec{x_i}, & m_{i,j}>0 \\\\\n",
    "            0, & \\mathrm{else} \\\\\n",
    "        \\end{cases}\n",
    "    ,~j \\neq y_i \\\\\n",
    "    \\\\\n",
    "    \\pderiv{L_i}{\\vec{w_{y_i}}} & = -\\vec{x_i} \\sum_{j\\neq y_i} \\mathbb{1}\\left( m_{i,j} > 0 \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $\\mathbb{1}(\\cdot)$ is an indicator function that takes the value $1$ if it's argument is a true statement, else it takes $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the hinge-loss function is not strictly speaking differentiable due to the $\\max$ operator.\n",
    "However, in practice it's not a major concern.\n",
    "Given that we know what argument the $\\max$ \"chooses\",\n",
    "we can differentiate each one of them separately.\n",
    "This is known as a **sub-gradient**.\n",
    "In the above, when $m_{i,j} \\leq 0$ we know the gradient will simply be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Based on the above, implement the gradient of the loss function in the module `hw1/losses.py`,\n",
    "within the `SVMHingeLoss` class.\n",
    "Implement the `grad()` function and complete what's missing in the `loss()` function.\n",
    "Make sure you understand the above gradient derivation before attempting to implement it.\n",
    "\n",
    "Note: you'll be implementing **only the first term** in the above equation for $\\pderiv{L}{\\vec{w_j}}(\\mat{W})$. We'll add the regularization term later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hinge-loss function\n",
    "loss_fn = SVMHingeLoss(delta=1)\n",
    "\n",
    "# Compute loss and gradient\n",
    "loss = loss_fn(x, y, x_scores, y_pred)\n",
    "grad = loss_fn.grad()\n",
    "\n",
    "# Sanity check only (not correctness): compare the shape of the gradient\n",
    "test.assertEqual(grad.shape, lin_cls.weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in the above we only checked the shape, how do we know if we've implemented the gradient correctly?\n",
    "\n",
    "One approach is to recall the formal definition of the derivative, i.e.\n",
    "$$\n",
    "f'(x)=\\lim_{h\\to 0} \\frac{f(x+h)-f(x)}{h}.\n",
    "$$\n",
    "Another way to put this is that for a *small enough* $h$,\n",
    "$$\n",
    "f(x+h)\\approx f(x)+f'(x)\\cdot h.\n",
    "$$\n",
    "\n",
    "We can use this approach to implement a gradient check by applying very small perturbations\n",
    "of each weight (separately) and using the above formula to check the correctness of the gradient\n",
    "(up to some tolerance). This is called a **numerical** gradient check.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll use a different approach, just to get a taste of the concept of\n",
    "**automatic differentiation**, which we'll rely on heavily in the rest of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simple linear model we worked with, the gradient was fairly straightforward to derive analytically\n",
    "and implement.\n",
    "However for complex models such as deep neural networks with many layers and non-linear operations between\n",
    "them this is not the case. Additionally, the gradient must be re-derived any time either the model\n",
    "architecture or the loss function changes. These things make it infeasible in practice to perform\n",
    "deep-learning research using this manual method of gradient derivation.\n",
    "Therefore, all deep-learning frameworks provide a mechanism of [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), to prevent\n",
    "the user from needing to manually derive the gradients of loss functions.\n",
    "\n",
    "`PyTorch` provides this functionality in a package named `torch.autograd` which we will use further on in the\n",
    "next exercises.\n",
    "For now, here's an example showing that autograd can compute the gradient of the loss function you've implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Run the following code block. Try to understand how autograd is used and why. If the test fails, go back and fix your gradient calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 8.96107292175293\n",
      "grad =\n",
      " tensor([[ 0.1500, -0.2600, -0.1600,  ...,  0.0100,  0.1100,  0.0600],\n",
      "        [-0.0636,  0.1103,  0.0679,  ..., -0.0042, -0.0467, -0.0255],\n",
      "        [-0.0636,  0.1103,  0.0679,  ..., -0.0042, -0.0467, -0.0255],\n",
      "        ...,\n",
      "        [-0.0636,  0.1103,  0.0679,  ..., -0.0042, -0.0467, -0.0255],\n",
      "        [-0.0636,  0.1103,  0.0679,  ..., -0.0042, -0.0467, -0.0255],\n",
      "        [-0.0636,  0.1103,  0.0679,  ..., -0.0042, -0.0467, -0.0255]])\n",
      "autograd =\n",
      " tensor([[ 0.1500, -0.2600, -0.1600,  ...,  0.0100,  0.1100,  0.0600],\n",
      "        [-0.0636,  0.1103,  0.0679,  ..., -0.0042, -0.0467, -0.0255],\n",
      "        [-0.0636,  0.1103,  0.0679,  ..., -0.0042, -0.0467, -0.0255],\n",
      "        ...,\n",
      "        [-0.0636,  0.1103,  0.0679,  ..., -0.0042, -0.0467, -0.0255],\n",
      "        [-0.0636,  0.1103,  0.0679,  ..., -0.0042, -0.0467, -0.0255],\n",
      "        [-0.0636,  0.1103,  0.0679,  ..., -0.0042, -0.0467, -0.0255]])\n",
      "diff = 4.3655032641254365e-05\n"
     ]
    }
   ],
   "source": [
    "from hw1.losses import SVMHingeLoss\n",
    "\n",
    "# Create a new classifier and loss function\n",
    "lin_cls = hw1linear.LinearClassifier(n_features, n_classes)\n",
    "loss_fn = SVMHingeLoss(delta=1)\n",
    "\n",
    "# Specify we want the gradient to be saved for the weights tensor\n",
    "# (just for our test)\n",
    "lin_cls.weights.requires_grad = True\n",
    "\n",
    "# Forward pass using the weights tensor, calculations will be tracked\n",
    "y_pred, x_scores = lin_cls.predict(x)\n",
    "\n",
    "# Compute loss of predictions and their analytic gradient\n",
    "loss = loss_fn(x, y, x_scores, y_pred)\n",
    "grad = loss_fn.grad()\n",
    "\n",
    "# Compute gradient with autograd\n",
    "autograd_grad = torch.autograd.grad(loss, lin_cls.weights)[0]\n",
    "\n",
    "# Calculate the difference between analytic and autograd\n",
    "diff = torch.norm(grad - autograd_grad).item()\n",
    "print('loss =', loss.item())\n",
    "print('grad =\\n', grad)\n",
    "print('autograd =\\n', autograd_grad)\n",
    "print('diff =', diff)\n",
    "\n",
    "test.assertLess(diff, 1e-3, \"Gradient diff was too large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model with SGD\n",
    "<a id=part3_4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, solving a machine-learning problem requires defining the following components:\n",
    "- A model:\n",
    "  architecture (type of model) consisting of hyperparameters (e.g. number of hidden layers, number of classes, etc)\n",
    "  which are set in advance and trainable parameters which we want to fit to data.\n",
    "- A loss function (sometimes denoted as a criterion):\n",
    "  evaluates the model output on some data compared to ground truth.\n",
    "- An optimization scheme:\n",
    "  specifies how the model should be updated to improve the loss. May also have hyperparameters.\n",
    "- A dataset:\n",
    "  What to fit the model to. Usually the available data is split into training, validation and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented our loss function and it's gradient, we can finally train our model.\n",
    "\n",
    "Implementation notes:\n",
    "- You'll find that when implementing your solutions it's wise to keep the above components separate as to be\n",
    "  able to change each one of them independently from the other.\n",
    "- In this exercise we'll have separated the loss and dataset, however for simplicity we'll implement the\n",
    "  model and optimizer together.\n",
    "- As you'll see further on, `PyTorch` provides very effective mechanisms to implement all of\n",
    "  these components in a decoupled manner.\n",
    "- Note that our loss implementation **didn't include regularization**. We'll add this during the training phase\n",
    "  using the `weight_decay` parameter. The reason is that we prefer that the part of the loss which only depends\n",
    "  on the model parameters be part of the optimizer, not the loss function (though both ways are possible).\n",
    "  You'll see this pattern later on when you use `PyTorch`'s optimizers in the `torch.optim` package.\n",
    "- In practice we use batches of samples from the training set when training the model, because usually the training\n",
    "  set can't fit into memory. Using gradients computed on batches of data at a time is known as mini-batch\n",
    "  stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** \n",
    "1. Implement the model training loop in the `LinearClassifier`'s `train()` method.\n",
    "   Use mini-batch SGD for the weight update rule.\n",
    "1. Update the training hyperparameters in the `hyperparams` function.\n",
    "   You should play with the hyperparameters to get a feel for what they do to the\n",
    "   loss and accuracy graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparams = {'weight_std': 0.005, 'learn_rate': 0.005, 'weight_decay': 0.005}\n",
      "Training..............................\n",
      "Test-set accuracy before training: 13.5%\n",
      "Test-set accuracy after training: 86.9%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hp = hw1linear.hyperparams()\n",
    "print('hyperparams =', hp)\n",
    "\n",
    "lin_cls = hw1linear.LinearClassifier(n_features, n_classes, weight_std=hp['weight_std'])\n",
    "\n",
    "# Evaluate on the test set\n",
    "x_test, y_test = dl_utils.flatten(dl_test)\n",
    "y_test_pred , _= lin_cls.predict(x_test)\n",
    "test_acc_before = lin_cls.evaluate_accuracy(y_test, y_test_pred)\n",
    "\n",
    "# Train the model\n",
    "svm_loss_fn = SVMHingeLoss()\n",
    "train_res, valid_res = lin_cls.train(dl_train, dl_valid, svm_loss_fn,\n",
    "                                    learn_rate=hp['learn_rate'], weight_decay=hp['weight_decay'],\n",
    "                                    max_epochs=30)\n",
    "\n",
    "# Re-evaluate on the test set\n",
    "y_test_pred , _= lin_cls.predict(x_test)\n",
    "test_acc_after = lin_cls.evaluate_accuracy(y_test, y_test_pred)\n",
    "\n",
    "# Plot loss and accuracy\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "for i, loss_acc in enumerate(('loss', 'accuracy')):\n",
    "    axes[i].plot(getattr(train_res, loss_acc))\n",
    "    axes[i].plot(getattr(valid_res, loss_acc))\n",
    "    axes[i].set_title(loss_acc.capitalize(), fontweight='bold')\n",
    "    axes[i].set_xlabel('Epoch')\n",
    "    axes[i].legend(('train', 'valid'))\n",
    "    axes[i].grid(which='both', axis='y')\n",
    "    \n",
    "# Check test set accuracy\n",
    "print(f'Test-set accuracy before training: {test_acc_before:.1f}%')\n",
    "print(f'Test-set accuracy after training: {test_acc_after:.1f}%')\n",
    "test.assertGreaterEqual(test_acc_after, 85.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this is a very nave model, you should get at least 85% test set accuracy if you implemented training correctly. You can try to change the hyperparameters and see whether you get better results. Generally this should be done with cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to understand what models learn is to try to visualize their learned parameters.\n",
    "There can be many ways to do this. Let's try a very simple one, which is to reshape them into images of the input\n",
    "size and see what they look like.\n",
    "\n",
    "**TODO** Implement the `weights_as_images()` function in the `LinearClassifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2108/1491828918.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcs3600\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mw_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlin_cls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_as_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors_as_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\deep-learning\\hw1\\hw1\\linear_classifier.py\u001b[0m in \u001b[0;36mweights_as_images\u001b[1;34m(self, img_shape, has_bias)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# ========================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mw_images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w_images' is not defined"
     ]
    }
   ],
   "source": [
    "import cs3600.plot as plot\n",
    "\n",
    "w_images = lin_cls.weights_as_images(img_shape=(1,28,28))\n",
    "fig, axes = plot.tensors_as_images(list(w_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can better understand the model by plotting some samples and looking at wrong predictions.\n",
    "Run the following block to visualize some test-set examples and the model's predictions for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the test set and their predictions\n",
    "n_plot = 104\n",
    "x_test, y_test = next(iter(dl_test))\n",
    "x_test = x_test[0:n_plot]\n",
    "y_test = y_test[0:n_plot]\n",
    "y_test_pred, _ = lin_cls.predict(x_test)\n",
    "x_test_img = torch.reshape(x_test[:, :-1], (n_plot, 1, 28, 28))\n",
    "\n",
    "fig, axes = plot.tensors_as_images(list(x_test_img), titles=y_test_pred.numpy(),\n",
    "                                   nrows=8, hspace=0.5, figsize=(10,8), cmap='gray')\n",
    "\n",
    "# Highlight the wrong predictions\n",
    "wrong_pred = y_test_pred != y_test\n",
    "wrong_pred_axes = axes.ravel()[wrong_pred.numpy().astype(np.bool)]\n",
    "for ax in wrong_pred_axes:\n",
    "    ax.title.set_color('red')\n",
    "    ax.title.set_fontweight('bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw1/answers.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs3600.answers import display_answer\n",
    "import hw1.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 \n",
    "\n",
    "Explain why the selection of $\\Delta > 0$ is arbitrary for the SVM loss $L(\\mat{W})$ as it is defined above (the full in-sample loss, with the regularization term).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_answer(hw1.answers.part3_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Given the images in the visualization section above,\n",
    "\n",
    "1. How do you interpret what the linear model is actually learning? Can you explain some of the classification\n",
    "   errors based on it?\n",
    "1. How is this interpretation similar or different from KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_answer(hw1.answers.part3_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "1. Based on the graph of the training set loss, would you say that the learning rate you chose is:\n",
    "    - Too low\n",
    "    - Good\n",
    "    - Too High\n",
    "    \n",
    "  Explain your answer by describing what the loss graph would look like in the other two cases when training\n",
    "  for the same number of epochs.\n",
    "  \n",
    "1. Based on the graph of the training and test set accuracy, would you say that the model is:\n",
    "    - Slightly overfitted to the training set\n",
    "    - Highly overfitted to the training set\n",
    "    - Slightly underfitted to the training set\n",
    "    - Highly underfitted to the training set\n",
    "    \n",
    "  and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_answer(hw1.answers.part3_q3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
